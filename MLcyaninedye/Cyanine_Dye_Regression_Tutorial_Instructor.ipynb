{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQVGdgWdoB4T"
      },
      "source": [
        "Version Date: August 3, 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO-mxkfb8xSk"
      },
      "source": [
        "# Part I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b47HHCQ5VVqQ"
      },
      "source": [
        "# Objectives\n",
        "\n",
        "- Build regression models that predict the wavelength of maximum absorbance of different cyanine dyes.\n",
        "- Understand and apply different types of regression models, including simple linear regression, multiple regression, penalized regression, and tree models.\n",
        "- Use feature selection and the evaluation of feature importance to improve and interpret model performance.\n",
        "- Use regression model analysis as part of the scientific discovery process by generating hypotheses for factors that affect cyanine dye absorption.\n",
        "- Gain proficiency in reading and writing Python code in the Google Colab/Jupyter notebook environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjzrzkQ3X0HB"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "- **If you have limited Google Colab experience,**  you may wish to watch this screencast by watching this [brief (3 minute) screencast](https://www.youtube.com/watch?v=inN8seMm7UI) and/or [reading this (1 page) introductory explanation](https://colab.research.google.com/notebooks/intro.ipynb) which will help you get started by introducing basic concepts and functionalities.\n",
        "\n",
        "Colab notebooks consist of text cells (like this one) and program code cells, like the ones below. A code cell is evaluated by typing the **Cmd+Enter** keys (or **Shift+Enter**). You can also execute a code cell by mousing over the `[ ]` symbol in the upper left-hand side of the code cell---when you hover over it, it will turn into a \"play\" button, and clicking the play button will execute the code cell. You can find other options for executing groups of cells in the \"Runtime\" menu above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPi_kgRTE7Py"
      },
      "outputs": [],
      "source": [
        "# This is a code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYxe2rHPXlEJ"
      },
      "source": [
        "### Basics of Python\n",
        "Like other programming languages, Python includes libraries, variables and functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGM1_mhbnX3i"
      },
      "source": [
        "### Loading Python Libraries\n",
        "\n",
        "Take a moment to look at this code block:\n",
        "- `import` loads a module\n",
        "- `import ... as` allows you to assign a short alias to the module\n",
        "- `from ... import` loads a small portion of a module\n",
        "- observe that the `import`, `as` and `from` keywords are color coded purple.  \n",
        "- `#` indicates a comment (observe that all of the text following the `#` is color coded green).  This text is not interpreted by the computer, and its goal is to provide the human with some information about what is happening.  \n",
        "\n",
        "What do each of these program modules do?  You can think of them as being like a library of books that accomplish program tasks.  In general, they can be quite complicated.  In most cases, you will never learn all of the functionality of a module, and will have to use the documentation to help you determine the relevant parts for solving your problem.  It is useful to have a general sense of the types of tasks that each of modules do, so that you can find the appropriate functionality.\n",
        "\n",
        "- [pandas](http://pandas.pydata.org) is a library for handling datasets\n",
        "- [numpy](https://numpy.org/) and [scipy](https://www.scipy.org/) are libraries for mathematical and scientific computing\n",
        "- [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/) are libraries for data visualization\n",
        "- [sklearn](https://scikit-learn.org/stable/) is a library for machine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-r_kUFgXy35"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.axes as ax\n",
        "\n",
        "#to split train and test dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For Machine Learning Models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "#to split train and test dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#for model evaluations\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# for feature selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# for reproducibility\n",
        "random_seed = 1841\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUWF4d7X0GFP"
      },
      "source": [
        "### Installing RDKit Module\n",
        "- To visualize molecular structures, we will use the `RDKit` [module](https://www.rdkit.org/)\n",
        "- The two code blocks below will install RDKit in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoR5qLlr0HWu"
      },
      "outputs": [],
      "source": [
        "# to install rdkit-pypi library to visualize molecular structures\n",
        "import sys\n",
        "!time pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlkUWNSt03fK"
      },
      "outputs": [],
      "source": [
        "# import rdkit modules\n",
        "try:\n",
        "  from rdkit import Chem\n",
        "  from rdkit.Chem import Draw\n",
        "  from rdkit.Chem.Draw import IPythonConsole\n",
        "except ImportError:\n",
        "  print('Stopping RUNTIME. Colaboratory will restart automatically. Please run again.')\n",
        "  exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGT17c-XsY0S"
      },
      "source": [
        "### Variables and Functions\n",
        "\n",
        "**Variables** are reserved memory locations that store values.  Think of variables like a container that hold data which can be changed later in the program. For example to create a variable named `number` and assign its value as `100`:\n",
        "\n",
        "```\n",
        "    number = 100\n",
        "```\n",
        "\n",
        "This variable can be modified at any time.\n",
        "```\n",
        "    number = 99\n",
        "    number = 1\n",
        "```\n",
        "\n",
        "The value of `number` has changed to 1.\n",
        "\n",
        "**Functions** are sets of operations that take an action on some input. Functions are defined using the `def` keyword. Functions only run when they are called, and the names of user-provided input values are called **arguments**.\n",
        "\n",
        "For example, let's define an `AbsoluteValue` function as below, which takes one argument, the number for which the absolute value should be calculated.\n",
        "```\n",
        "def AbsoluteValue(num):\n",
        "    if num >= 0:\n",
        "        return num\n",
        "    else:\n",
        "        return -num\n",
        "```\n",
        "The output of `AbsoluteValue(2)` is `2`, and `AbsoluteValue(-4)` is `4`.\n",
        "\n",
        "We can also write nested operations such as:\n",
        "```\n",
        "np.sqrt(np.exp(abs(np.sin(np.tan(-0.2)))))\n",
        "```\n",
        "The output of `np.sqrt(np.exp(abs(np.sin(np.tan(-0.2)))))` is `1.1059`\n",
        "\n",
        "### Advanced topics\n",
        "\n",
        "The code below uses some other features of variables in Python.  It is not necessary to review these now, but you may want to refer to this if you need to modify the code.\n",
        "\n",
        "[Lists](https://developers.google.com/edu/python/lists) can be used to store a series of values. The list is defined by the square brackets, `[]`. Each entry in a [list](https://developers.google.com/edu/python/lists) has an address that can be used to identify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NswhJJBcquot"
      },
      "outputs": [],
      "source": [
        "numberList  = [1, 2, -1, 1, -2.2, -42] # here is way to declare a list\n",
        "print(numberList) # print the list\n",
        "numberList[4] == -2.2 # square brackets used to access item in a list by index. Indexes start with 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD5iViXe0c4N"
      },
      "source": [
        "[Dictionaries](https://developers.google.com/edu/python/dict-files) organize groups of entries in terms of key-value pairs; this is especially useful when there is no numerical ordering implied, but we still want to group data together. The dictionary is defined by the curly brackets, `{}`, and using the sytaxis `key:value`. For example, our keys will be names of fruits in English, and the values are the corresponding names in Spanish:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9uYY7QyucN6"
      },
      "outputs": [],
      "source": [
        "translate = {\"apple\" : \"manzana\", \"orange\" : \"naranja\"} # define a dictionary\n",
        "translate[\"apple\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSiNacWs5yq0"
      },
      "source": [
        "We can also access the keys and values arrays by calling the relevant attributes of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsH00a4Q5qbf"
      },
      "outputs": [],
      "source": [
        "print(translate.keys())\n",
        "print(translate.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LbxW6isKPpb"
      },
      "source": [
        "## Get and Preprocess the Data\n",
        "Now let's load in the train and test datasets, which are stored on [GitHub](https://docs.github.com/en/get-started/quickstart/hello-world#introduction) (a repository for storing and sharing data and software code). To do this, we will need to use a **module**.\n",
        "Using a built-in method of a module is carried out by writing: `[module_name].[method]`\n",
        "\n",
        "For example, to use the `read_csv` method of the `pandas` module: `pd.read_csv()`.  With this method, CSV files are read as a **DataFrame** structure, which is similar to a table. For more on DataFrames:\n",
        "- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/03.01-introducing-pandas-objects.html)\n",
        "- [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) documentation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYeb5qcd2ZOI"
      },
      "source": [
        "### Load and Visualize the Data\n",
        "\n",
        "First let's load the data from a GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the chem dataset from GitHub into pandas dataframes\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/elizabeththrall/MLforPChem/main/MLcyaninedye/Data/cyaninedye_dataset.csv\")"
      ],
      "metadata": {
        "id": "oSMyziOdoIz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcawQr6jYEcq"
      },
      "source": [
        "(Note that `pandas.read_csv()` can be used to load data from anywhere, including files on your local computer; here we are reading it from an internet URL).\n",
        "\n",
        "Now let's see what these data look like. You can display the current contents of a variable by entering its name and executing the cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ykgSfvSdH3a"
      },
      "outputs": [],
      "source": [
        "# display the contents of the variable \"dataset\"\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ErQtE73XNkG"
      },
      "source": [
        "Take a look at the structure of this variable (you may need to use the scroll buttons to see each row and column, depending on your screen size):\n",
        "\n",
        "* Each row contains data for a different molecule\n",
        "* The numbers to the left of the first column (0, 1, ...) represent the index of each row\n",
        "* The first column contains the molecule SMILES string. SMILES is a representation of the atoms and the bonds between them in the form of an ASCII string.\n",
        "* The second column contains the molecule InChIKey. InChIKey is another unique chemical identifier in the form of an ASCII string.\n",
        "* The next 13 columns contain different molecular features (described below). These are the variables that we will use to predict the wavelength of maximum absorbance. Features are sometimes called \"independent\" or \"predictor\" variables.\n",
        "* The final column contains the wavelength of maximum absorbance (MaxAbsorbanceWavelength) for each molecule. This is the target variable that we want to predict (sometimes called the \"dependent\" or \"response\" variable).\n",
        "\n",
        "**Features:**\n",
        "\n",
        "Our regression analysis will use the 13 features below, which were computed for each molecule in the dataset.\n",
        "- AromaticRingCount: Number of aromatic rings in the molecule\n",
        "- HBondDonorCount: Number of hydrogen bond donors in the molecule\n",
        "- HBondAcceptorCount: Number of hydrogen bond acceptors in the molecule\n",
        "- FractionCarbonSP3: Fraction of carbon atoms in the molecule with sp3 hybridization\n",
        "- HeteroatomCount: Total number of heteroatoms (i.e., atoms other than carbon) in cyclic rings in the molecule\n",
        "- HeterocycleCount: Total number of heterocyclic rings (i.e., rings containing atoms other than carbon) in the molecule\n",
        "- RotatableBondCount: Number of bonds that can be rotated in the molecule (i.e., the number of non-cyclic sp3 bonds)\n",
        "- MolecularMass: Mass of the molecule\n",
        "- DegreeOfUnsaturation: Total number of pi bonds and rings in the molecule\n",
        "- MinEllipsoidLength: The major axis length of the smallest volume ellipsoid that could contain the molecule\n",
        "- MaxDistance: The largest distance between any pair of atoms in the molecule\n",
        "- LongestPiChain: Longest continuous path of conjugated pi bonds\n",
        "- LinkerLength: Longest conjugated pi chain that is not part of a ring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxa43i3aBahj"
      },
      "source": [
        "### Data Selection with Pandas\n",
        "A Pandas dataset can be thought of as storing a collection of values in a rectangular \"grid.\"  We can see the size of that grid using the `shape()` class atribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VqWrjqjBycT"
      },
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnqPR19QbpvQ"
      },
      "source": [
        "We will often need to access the values stored in particular positions in a variable. We can do this by specifying the indices corresponding to that position:\n",
        "- `iloc[row index, column index] `extracts the value at the specified row and column.\n",
        "- `:` specifies a range of values\n",
        "- Note that in Python, index values start from `0` instead of `1`\n",
        "\n",
        "For example:\n",
        "- `iloc[1:3,0]` : select row indices 1 to 2 (i.e., second and third rows) and the first column\n",
        "- `iloc[:,0]` : select all rows and the first column\n",
        "- `iloc[:,2:5]`: select all rows and column indices 2 to 4 (i.e., third through fifth columns)\n",
        "\n",
        "Try the following examples. Can you predict what the ouput will be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2lc-guHLuRo"
      },
      "outputs": [],
      "source": [
        "# this line of code access the first entry by its indices\n",
        "dataset.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jffNScZbzZI"
      },
      "outputs": [],
      "source": [
        "# this line of code returns the first row and first column of the training data\n",
        "dataset.iloc[0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUrb9gz5b8NA"
      },
      "outputs": [],
      "source": [
        "# this line of code returns the first three rows and first 10 columns of the training data\n",
        "dataset.iloc[0:3,0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk4Mli7fcDau"
      },
      "outputs": [],
      "source": [
        "# guess what the output of this line of code will be\n",
        "dataset.iloc[0:3,0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2YSNP1fNj7l"
      },
      "outputs": [],
      "source": [
        "# This can also be generalized to select a collection of columns by providing a list as input\n",
        "dataset[[\"MolecularMass\", \"MaxAbsorbanceWavelength\"]].iloc[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdOWMmg8zHM3"
      },
      "source": [
        "### Visualizing molecular structure\n",
        "\n",
        "To continue familiarizing ourselves with the dataset, we can visualize the structures of the different cyanine dyes.  Before we do so, let's first get a quick overview of making and plotting molecules in general in Python. The [RDKit](https://www.rdkit.org) Python library helps us to do this task. This library can take input names (i.e., machine-readable SMILES) and convert them into a two dimensional representation.\n",
        "\n",
        "\n",
        "Change the `molecule` value below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqOKVcV5Qn5J"
      },
      "outputs": [],
      "source": [
        "molecule = \"CN(C)C(=N)N(C)C\"\n",
        "\n",
        "# RDKit takes a Molecule as an input and returns a two-dimensional figure:\n",
        "mol = Chem.MolFromSmiles(molecule)\n",
        "Chem.Draw.MolToImage(mol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu7RDUV-SkgL"
      },
      "source": [
        "Now let's use these tools to visualize some of the molecules in our dataset.  The code block below displays the structure of the molecules in the rows of our dataset corresponding to the specified index. Try changing the values in `idx_molecules` to visualize the structure of a few different molecules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dj066UzzbsD"
      },
      "outputs": [],
      "source": [
        "# change the index values below to pick which structure to display\n",
        "idx_molecules=[0, 1, 2, 3, 4]\n",
        "\n",
        "# get the SMILES strings\n",
        "molecules = dataset.iloc[idx_molecules,0].values\n",
        "molecules = [Chem.MolFromSmiles(smiles) for smiles in molecules]\n",
        "# display the molecular structures\n",
        "img1=Chem.Draw.MolsToGridImage(molecules,molsPerRow=5,subImgSize=(200,200), legends=[f'Molecule idx: {idx}' for idx in idx_molecules])\n",
        "img1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrdlT6hdzDbB"
      },
      "source": [
        "###Handling Data with Outliers\n",
        "\n",
        "Sometimes removing outliers from the dataset can help improve model performance. Outliers are data points that differ drastically from the rest of the observations. Machine Learning models generalize from training data to make predictions; if they learn from data with extreme outliers, they may perform worse.\n",
        "\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Outlier)\n",
        "\n",
        "It is always useful to identify possible outliers by charting a histogram of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adZmQ3wmGSVS"
      },
      "outputs": [],
      "source": [
        "plt.hist(dataset[\"MaxAbsorbanceWavelength\"], bins=20, alpha=0.7,color='orange')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Sample Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY1ri-FUGViN"
      },
      "source": [
        "Specifically, for our exercise, we will only consider the observations where the `MaxAbsorbanceWavelength` is in the 90% quantile or below. This way, we will avoid extreme data points in our studied variable.\n",
        "\n",
        "(*Quantile*: If we divide the whole data distribution into equal-sized, ordered cuts, the quantiles represent the corresponding subgroups. Let's imagine our variable goes from 1 to 10; if we state we want to keep the quantile 90%, we will only study the data points where the variable is between 1 and 9.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu5VEuOL99ga"
      },
      "outputs": [],
      "source": [
        "# Remove outliers\n",
        "dataset = dataset[dataset['MaxAbsorbanceWavelength'] <= np.quantile(dataset['MaxAbsorbanceWavelength'], 0.90)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jrmQU31RH6_"
      },
      "source": [
        "### Identifying Dependent (y) and Independent Variables (X)\n",
        "\n",
        "Next we need to split our dataset into dependent and independent variables. An independent variable does not depend on other variables, whereas a dependent variable is affected by independent variables and changes accordingly. Independent and dependent variables are commonly indicated as X and y:\n",
        "\n",
        "- X is an array of independent variables\n",
        "- y is the dependent variable\n",
        "\n",
        "In regression analysis, the dependent variable that we want to predict is referred to as a target value. In our dataset, `MaxAbsorbanceWavelength` is the target value (i.e., the dependent variable, y), and the 13 features are the independent variables (i.e., X). We will omit the non-numerical variables in our dataset (the *SMILES* and *InChIKey* molecular identifiers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW5a2G9_gBZQ"
      },
      "outputs": [],
      "source": [
        "X = dataset.iloc[:,0:-1] # Independent columns\n",
        "X.drop(['SMILES', 'InChIKey'], axis=1, inplace=True) # Eliminate non-numerical features\n",
        "y = dataset.iloc[:,-1] # Target column (MaxAbsorbanceWavelength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-podlq9jlLSU"
      },
      "source": [
        "### Split Train & Test Data\n",
        "\n",
        "We will use most of our dataset to train our regression models; this portion is the \"training\" dataset. To evaluate model performance, however, we will need a test data set.  We will randomly split our data into a training and test set using the `train_test_split` module of `sklearn` library. By default this function does an 80/20% split with a random shuffle of the items. (In order to make this split reproducible, we defined a random number generator \"seed\" above, ensuring that you get the same random result each time you run the notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPPnelxtlRVj"
      },
      "outputs": [],
      "source": [
        "# split our dataset into two sets: training dataset and test dataset, keeping 20% of the data for testing\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size =0.20, random_state = random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW1rA1YPMKfc"
      },
      "outputs": [],
      "source": [
        "# display Xtrain set\n",
        "Xtrain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4TOpp8bMbDJ"
      },
      "outputs": [],
      "source": [
        "# display Xtest set\n",
        "Xtest.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrYVtqBPr0JP"
      },
      "source": [
        "### Data Scaling\n",
        "Notice that our features span a wide range of different values, from less than 1 to greater than 100. Many machine learning models perform better when the variables are on the same scale, so it's usually a good practice to rescale features before performing the analysis.\n",
        "\n",
        "*   [Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling)\n",
        "\n",
        "There are many methods that can be used to scale data. Two approaches are:\n",
        "\n",
        "*   Scaling: Scale our features in a specified range (e.g., between 0 and 1) without altering the distribution shape; for example, [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
        "*   Normalizing: Scale our features, usually altering the distribution shape. Some techniques are [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard%20scaler#sklearn-preprocessing-standardscaler) and [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html?highlight=robust%20scaler#sklearn-preprocessing-robustscaler).\n",
        "\n",
        "For this analysis, let's scale all of our features to range between 0 and 1, based on the minimum and maximum of the training set. The functions `MinMaxScaler`, defined below, accomplish this task. You do not need to modify these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxi6DDOltLOC"
      },
      "outputs": [],
      "source": [
        "# import the MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# We begin by determining the maximum and minimum of each feature:\n",
        "XtrainScaled = scaler.fit_transform(Xtrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7xWbz7sMvxj"
      },
      "source": [
        "Then we can apply these functions to transform the training and test sets to have input features between 0 and 1.  (**Be careful**,  as this will replace the values in these datasets with the rescaled versions. **You should only run this code once**. If you accidentally run it twice, go back and regenerate the training and test sets in the section [Split Train & Test Data](#scrollTo=-podlq9jlLSU&line=5&uniqifier=1).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q0bd-hyMu7Z"
      },
      "outputs": [],
      "source": [
        "XtestScaled = scaler.transform(Xtest)\n",
        "Xtrain = pd.DataFrame(XtrainScaled, index=Xtrain.index, columns= Xtrain.columns)\n",
        "Xtest = pd.DataFrame(XtestScaled, index=Xtest.index, columns= Xtest.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRumu2F9LLfS"
      },
      "outputs": [],
      "source": [
        "# Visualize the first few as an example\n",
        "Xtrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDNhOpmlLQgX"
      },
      "source": [
        "Note that none of the feature values in the training dataset are smaller than 0 or greater than 1 now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XeaNuO5Dble"
      },
      "source": [
        "## Building Regression Models\n",
        "\n",
        "Regression analysis is a process of explaining and defining the relationship between dependent and independent variables. In this activity, we will train several regression models to predict our target value, `MaxAbsorbanceWavelength` (in [this section](#scrollTo=8XeaNuO5Dble&line=3&uniqifier=1) and later in the [Penalized Regression](#scrollTo=QvWQHytwEeb6&line=1&uniqifier=1) and [Tree Models](#scrollTo=COTNAsOkGdXF) sections). We will use different metrics to evaluate how well the models perform (in the [Model Evaluation](#scrollTo=4i7by65Xx4gX) section). By choosing better independent variables (in the [Feature Selection](#scrollTo=B-I9vEAo8fJf) section) we can try to improve our model performance. Finally, we can interpret our models and gain insight into what features are important (in the [Feature Importance](#scrollTo=q8ulRc1uJ6kd&line=1&uniqifier=1) section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em8QB_1WDd8y"
      },
      "source": [
        "### Simple Linear Regression\n",
        "Let's start with the most basic type of regression model. **Simple linear regression** is a method that tries to define the relationship between two variables, the dependent variable and a single independent variable.\n",
        "\n",
        "*   [Wikipedia](https://en.wikipedia.org/wiki/Simple_linear_regression)\n",
        "*   [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html)\n",
        "\n",
        "This relationship can be represented with a well-fitted line that follows:\n",
        "$$\n",
        "\\hat{Y}=β_0+β_1X\n",
        "$$\n",
        "where:\n",
        "* $β_0$: $y$ intercept\n",
        "* $β_1$: slope (called the coefficient or weight in this context)\n",
        "\n",
        "The optimal slope is generally determined by minimizing the residual sum of squares ([RSS](https://en.wikipedia.org/wiki/Residual_sum_of_squares)), which is given by:\n",
        "\n",
        "$$\n",
        "RSS = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "where $y_i$ are the actual values and $\\hat{y}_i$ are the model predictions. This approach is referred to as the ordinary least squares (OLS) method.  You can read more about these methods at the following links:\n",
        "\n",
        "*   [Residual sum of squares](https://en.wikipedia.org/wiki/Residual_sum_of_squares)\n",
        "*   [Ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n",
        "\n",
        "Now that we know how a simple linear regression model works, let's train our first machine learning model by selecting one feature from our set of independent variables and plotting the line that best describes the relationship between our selected feature and the dependent variable (`MaxAbsorbanceWavelength`).\n",
        "\n",
        "Let's start by choosing the `MolecularMass` feature and seeing how well it predicts the `MaxAbsorbanceWavelength`. Run the code block below to select the feature, extract the training and testing data for that feature, and train the regression model. (Note that to run a simple linear regression with another feature, you can simply copy this code block and change the `featureName` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac-OWuAMj7G6"
      },
      "outputs": [],
      "source": [
        "# Function to plot a scatter graph to show the predictions\n",
        "# results - 'ytrue' value vs 'y_pred' value\n",
        "def ResultsScatterPlot(predictions, ytest, title):\n",
        "  x = np.linspace(360, 800,100)\n",
        "  y = x\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.plot(x, y, '-b')\n",
        "  plt.scatter(predictions, ytest, color = 'limegreen')\n",
        "  diffs_ = (ytest - predictions)\n",
        "  diffs_positive = diffs_.apply(lambda x: x if x > 0 else 0)\n",
        "  diffs_negative = diffs_.apply(lambda x: abs(x) if x < 0 else 0)\n",
        "  plt.errorbar(predictions, ytest, yerr=[diffs_positive, diffs_negative], fmt='.', ecolor='red')\n",
        "  # axis labels\n",
        "  plt.xlabel(\"Predicted value\")\n",
        "  plt.ylabel(\"Actual Value\")\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KdGER9guQN"
      },
      "outputs": [],
      "source": [
        "# select a single feature\n",
        "featureName = 'MolecularMass'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(Xtrain[featureName].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(Xtest[featureName].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(single_feature_lr_X_train, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting the test set results\n",
        "y_pred_simple_lr = regressor.predict(single_feature_lr_X_train)\n",
        "\n",
        "# Plotting Scatter graph to show the prediction\n",
        "title = f\"True Wavelength vs. Predicted: Linear Regression - Using {featureName}\"\n",
        "ResultsScatterPlot(y_pred_simple_lr, ytrain, title)"
      ],
      "metadata": {
        "id": "ghU0O7iMqCJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA8iLxCjjC2C"
      },
      "source": [
        "The graph above plots the actual wavelength vs. the predicted wavelength for the training data. If the model could perfectly explain the variation in the training data, all the dots would lie along the blue dashed line. Clearly the model is not perfect.\n",
        "\n",
        "It can also be useful to examine the correlation between the feature used to the train the regression model and the target value. Let's plot the actual MaxAbsorbanceWavelength as a function of the value of MolecularMass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4cBleAbTSAN"
      },
      "outputs": [],
      "source": [
        "# Plot a graphical representation\n",
        "plt.plot(single_feature_lr_X_train, ytrain, 'o', color='red')\n",
        "plt.plot(np.linspace(min(single_feature_lr_X_train), max(single_feature_lr_X_train),100),\n",
        "         regressor.intercept_+regressor.coef_ * np.linspace(min(single_feature_lr_X_train), max(single_feature_lr_X_train),100),\n",
        "         color='b')\n",
        "plt.xlabel('Single feature selected: {}'.format(featureName))\n",
        "plt.ylabel(\"MaxAbsorbanceWavelength (nm)\")\n",
        "plt.legend(['Train data', 'Model'])\n",
        "plt.title('Graphical representation of Simple Linear Regression');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx4D9Fvykkws"
      },
      "source": [
        "If MolecularMass could explain all the variation in MaxAbsorbanceWavelength, the red dots would lie along the blue line. Again we can see that the regression model is not perfect.\n",
        "\n",
        "Now that we've evaluated the model performance with the training dataset, let's use this model to predict `MaxAbsorbanceWavelength` for the test dataset and compare our predicted wavelengths to the true values. This is the real test of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiHOofCXg0wm"
      },
      "outputs": [],
      "source": [
        "# predicting the test set results\n",
        "y_pred_simple_lr = regressor.predict(single_feature_lr_X_test)\n",
        "\n",
        "# Plotting Scatter graph to show the prediction\n",
        "title = f\"True Wavelength vs. Predicted: Linear Regression - Using {featureName}\"\n",
        "ResultsScatterPlot(y_pred_simple_lr, ytest, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJlhkEm3sl1"
      },
      "source": [
        "If the model predictions were perfect, all the blue dots would lie along the dashed line. As we saw before, this model is not perfect. There are quantitative ways that we can evaluate the performance of the model, which we'll discuss soon.\n",
        "\n",
        "For now, go back to the description of the different molecular features in our dataset. Use your chemical intuition and think about which ones might be correlated with MaxAbsorbanceWavelength. Then write new code to perform and visualize a linear regression for a different variable. (Hint: you can copy the code blocks above and modify them to choose a different feature and save a new model. Name the variables singleFeatureTrain2, singleFeatureTest2, and regressor2 to avoid overwriting the previous model.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQEcWbDt7QJP"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO PERFORM A SIMPLE LINEAR REGRESSION FOR A DIFFERENT VARIABLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLkG_izhZe7M"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "\n",
        "featureName2 = 'LongestPiChain'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(Xtrain[featureName2].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(Xtest[featureName2].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressor2 = LinearRegression()\n",
        "regressor2.fit(single_feature_lr_X_train, ytrain)\n",
        "\n",
        "# predicting the test set results\n",
        "y_pred_simple_lr2 = regressor2.predict(single_feature_lr_X_test)\n",
        "\n",
        "# Plotting Scatter graph to show the prediction\n",
        "title = f\"True Wavelength vs. Predicted: Linear Regression - Using {featureName2}\"\n",
        "ResultsScatterPlot(y_pred_simple_lr2, ytest,title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehnDSpmd2NRV"
      },
      "source": [
        "### Multiple Regression\n",
        "\n",
        "When we have more than one independent variable in our dataset, we can make use of a **multiple regression model**. A multiple regression model can perform better at predicting the value of a dependent variable since the model has more information to use.\n",
        "\n",
        "*   [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression)\n",
        "\n",
        "The formula is an extension of the single-variable simple linear regression:\n",
        "$$\n",
        "\\hat{Y}=β_0+β_1X_1+β_2X_2+...+β_kX_k\n",
        "$$\n",
        "where:\n",
        "  * $β_0$: $y$ intercept\n",
        "  * $β_k$: slope corresponding to the k<sup>th</sup> feature\n",
        "  * $k$: Number of independent features used in the model\n",
        "\n",
        "As for the simple linear regression, the optimal slopes are determined by minimizing the RSS.\n",
        "\n",
        "Now let's perform a multiple linear regression using all 13 molecular features and plot the actual vs. predicted wavelength for the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kow5O3Ol5D1w"
      },
      "outputs": [],
      "source": [
        "# fit our linear reggression model\n",
        "regressor_mr = LinearRegression()\n",
        "regressor_mr.fit(Xtrain, ytrain)\n",
        "# predicting the test set results\n",
        "y_pred_lr = regressor_mr.predict(Xtest)\n",
        "\n",
        "# Plotting Scatter graph to show the prediction\n",
        "title = \"True Wavelength vs. Predicted: Multiple Regression\"\n",
        "ResultsScatterPlot(y_pred_lr, ytest, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjyOXv3Q7o1q"
      },
      "source": [
        "As for the single-variable case, perfect model performance would be indicated by all the blue dots lying along the dashed line. Although this model isn't perfect, its performance looks qualitatively better than the MolecularMass simple linear regression, and the predicted wavelengths span a wider range of values. Thus we can see that providing the model with more information improved the performance, as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i7by65Xx4gX"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "By plotting actual vs. predicted wavelength, we could make a qualitative assessment of model performance. But how can we quantify the performance of a particular model, or quantitatively compare two different models? To do this, we can evaluate performance metrics using our test data.\n",
        "\n",
        "Let's consider the following four metrics to quantify the agreement between our model predictions ($\\hat{y}_i$) and the actual values ($y_i$):\n",
        "\n",
        "- Mean Absolute Error (MAE): MAE is the average of the absolute difference between the actual value $y$ and the predicted value $\\hat{y}$. The lower the error, the better the model performance.\n",
        "$$\n",
        "MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\n",
        "$$\n",
        "  Where:\n",
        "  * $n$: number of data points/observations\n",
        "  * $y_i$: actual value\n",
        "  * $\\hat{y}_i$: predicted value\n",
        "\n",
        "  [Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
        "\n",
        "- Mean Squared Error (MSE): MSE is the average of the squared difference between the actual and predicted values. As before, the lower the error, the better the model performance. MSE is one of the most used metrics for this kind of model.\n",
        "$$\n",
        "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "  [Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
        "\n",
        "- Root Mean Squared Error (RMSE): RMSE is the square root of the MSE. As before, the lower the error, the better the model performance. It is conceptually similar to the standard deviation.\n",
        "$$\n",
        "RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\n",
        "$$\n",
        "  [Wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n",
        "\n",
        "- Coefficient of determination ($R^2$): $R^2$ reports how well the model is fitted to the data by comparing it to the average line of the dependent variable. In other words, it measures the degree to which the model explains the variability of the observed data. For example, a coefficient of determination of 80% indicates that the regression model explains 80% of the variability seen in the dependent variable.\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\n",
        "$$\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "  [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
        "\n",
        "\n",
        "Although these quantitative metrics are informative, it is a good practice to see how the data behave with our own eyes through data visualization. Sometimes a single number or metric cannot explain the whole picture, or can even be misleading. ([Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) is a famous illustration of that point.)\n",
        "\n",
        "We can request a single property using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZMDaHH3FbXT"
      },
      "outputs": [],
      "source": [
        "print(f\"MAE: {mean_absolute_error(ytest, y_pred_simple_lr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vo34xrJ3Er7"
      },
      "source": [
        "Let's make life easy by defining a convenience function that takes the model and a test set and calculates our four metrics of interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crcYRTjh1rxE"
      },
      "outputs": [],
      "source": [
        "# One single function for all metrics\n",
        "def ModelEvaluation(y_true, y_pred):\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  rmse = mean_squared_error(y_true, y_pred)**(1/2)\n",
        "  r2 = r2_score(y_true, y_pred)\n",
        "  return {'MAE': mae,'MSE': mse,'RMSE': rmse, 'R2': r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHADEOM0P0VY"
      },
      "source": [
        "Now we can store these results in a variable called metrics and add the results for the simple linear regression and multiple regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLpHnBxlPz-M"
      },
      "outputs": [],
      "source": [
        "# Calculate the performance metrics for the MolecularMass simple linear regression using the corresponding single - variable test data\n",
        "metrics_simple_lr = ModelEvaluation(ytest, y_pred_simple_lr)\n",
        "\n",
        "# Calculate the performance metrics for the multiple regression using the full test data\n",
        "metrics_multiple_lr = ModelEvaluation(ytest, y_pred_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87JM4XvxIACf"
      },
      "outputs": [],
      "source": [
        "# turn our metric results into a pandas dataset\n",
        "pd.DataFrame({f'Simple Linear Regression: {featureName}':metrics_simple_lr,\n",
        "              'Multiple Regression': metrics_multiple_lr})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-vLGJA6JCWK"
      },
      "source": [
        "When we compare the `MolecularMass` simple linear regression and the multiple regression, we can see that **multiple regression** works better according to both the eye test and the quantitative performance metrics.\n",
        "\n",
        "Now add a code block below to calculate these performance metrics for a simple linear regression with the single feature that you tested above, add them to the metrics variable, and display the results in table. How did your new model perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ich-9Gsb7nsm"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO CALCULATE THE PERFORMANCE METRICS FOR A SIMPLE LINEAR REGRESSION USING A DIFFERENT VARIABLE AND ADD THEM TO THE METRICS TABLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxG7Kuaacbm9"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "\n",
        "featureName2 = 'LongestPiChain'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(Xtrain[featureName2].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(Xtest[featureName2].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressor2 = LinearRegression()\n",
        "regressor2.fit(single_feature_lr_X_train, ytrain)\n",
        "\n",
        "# predicting the test set results\n",
        "y_pred_simple_lr2 = regressor2.predict(single_feature_lr_X_test)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "metrics_simple_lr2 = ModelEvaluation(ytest, y_pred_simple_lr2)\n",
        "pd.DataFrame({f'Simple Linear Regression: {featureName}':metrics_simple_lr,\n",
        "              'Multiple Regression': metrics_multiple_lr,\n",
        "              'Simple linear regression - New': metrics_simple_lr2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TVKJs_vyGZ5"
      },
      "source": [
        "##Avoiding Model Overfitting\n",
        "\n",
        "Overfitting refers to a situation when a model has become too specific to the training data and fails to generalize well to new test data. Overfitting generally occurs when there are too many variables in the model compared to the number of observations (i.e., the model is overly complicated and/or the training set is too small).\n",
        "\n",
        "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off)\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Overfitting)\n",
        "* [Scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) documentation\n",
        "\n",
        "There are a few ways to avoid overfitting. We'll try two approaches here:\n",
        "\n",
        "\n",
        "1.   Identifying the best correlated feature(s)\n",
        "2.   Using a penalized regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-I9vEAo8fJf"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Feature selection entails choosing better features for the model by eliminating some of the available independent variables. Reducing the number of features can help the algorithm perform better by eliminating misleading variables or preventing overfitting. For an overview, see the following article:\n",
        "\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Feature_selection)\n",
        "\n",
        "To identify appropriate features for our model, let's use the Pearson Correlation technique.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN2o6XyF8s82"
      },
      "source": [
        "### Pearson Correlation\n",
        "\n",
        "The Pearson Correlation Coefficient is one way to determine what values are relevant for our machine learning model. This value ranges from -1 to 1 and can be interpreted as follows:\n",
        "\n",
        "- If the value is exactly 0, it means no correlation at all\n",
        "- If value if closer to 0, it means weaker correlation.\n",
        "- If the value is closer to 1, it means stronger positive correlation\n",
        "- If the value is closer to -1, it means stronger negative correlation\n",
        "\n",
        "We are only concerned about the absolute value of the correlation as the direction is not relevant for this exercise.\n",
        "\n",
        "For more information on correlation, see:\n",
        "*   [Wikipedia](https://en.wikipedia.org/wiki/Correlation)\n",
        "\n",
        "Let's calculate the Pearson Correlation Coefficient for all 13 features in our dataset with the MaxAbsorbanceWavelength dependent variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsoPpI3MuXfr"
      },
      "outputs": [],
      "source": [
        "# Get pearson Correlation between all features\n",
        "P_correlation = pd.DataFrame(Xtrain.merge(ytrain, right_index=True, left_index=True).corr()['MaxAbsorbanceWavelength'].dropna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFgAfRiAvNTm"
      },
      "outputs": [],
      "source": [
        "# Display the absolute correlation scores\n",
        "P_corr = P_correlation.sort_values(by='MaxAbsorbanceWavelength', ascending=True)[:-1]\n",
        "corr_features = P_corr.index\n",
        "y_pos = np.arange(len(corr_features))\n",
        "corr_values = np.squeeze(P_corr.values)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.barh(corr_features, corr_values, alpha=0.7,color='orange');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHZWjPARyAVZ"
      },
      "outputs": [],
      "source": [
        "# Get the absolute value of the correlation scores\n",
        "P_correlation_abs = np.abs(P_correlation).sort_values(by='MaxAbsorbanceWavelength', ascending=False)[1:]\n",
        "P_correlation_abs.columns = ['MaxAbsorbanceWavelength Correlation']\n",
        "# Display the absolute correlation scores\n",
        "P_correlation_abs.style.background_gradient(cmap ='RdYlBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1b-h7i-V2e"
      },
      "source": [
        "Think about which features show strong positive or negative correlations. Does your chemical intuition say that any of these properties might be physically meaningful in determining cyanine dye absorption?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4J0HIw1dDWp"
      },
      "source": [
        "### Applying Pearson Correlation\n",
        "\n",
        "Now let's try training a simple linear regression model using only the feature with the highest correlation. Add code blocks below to train the model, plot the actual vs. predicted wavelengths, and generate performance metrics. (Hint: as before, you can do this by copying and modify code blocks from above.) How do the results compare to your previous simple linear regression models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFTxNNmFROBZ"
      },
      "source": [
        "**ADD A CODE BLOCK TO PERFORM A SIMPLE LINEAR REGRESSION FOR A SINGLE HIGHLY CORRELATED VARIABLE, PLOT THE RESULTS, AND CALCULATE PERFORMANCE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtwMUQtDfR7y"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "\n",
        "featureName3 = 'LinkerLength'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(Xtrain[featureName3].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(Xtest[featureName3].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressor3 = LinearRegression()\n",
        "regressor3.fit(single_feature_lr_X_train, ytrain)\n",
        "\n",
        "# predicting the test set results\n",
        "y_pred_simple_lr3 = regressor3.predict(single_feature_lr_X_test)\n",
        "\n",
        "# Calculate the performance metrics\n",
        "metrics_simple_lr3 = ModelEvaluation(ytest, y_pred_simple_lr3)\n",
        "pd.DataFrame({f'Simple Linear Regression: {featureName}':metrics_simple_lr,\n",
        "              'Multiple Regression': metrics_multiple_lr,\n",
        "              'Simple linear regression - featureName2': metrics_simple_lr2,\n",
        "              'Simple linear regression - featureName3': metrics_simple_lr3})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvWQHytwEeb6"
      },
      "source": [
        "## Regularized (or Penalized) Regression\n",
        "\n",
        "Another approach we can take to prevent overfitting is adding a tuning parameter (regularization) that penalizes model complexity. In other words, this penalty tends to keep the model coefficients smaller and minimizes the effect of extreme values in the training dataset. These types of models are referred to as regularized or penalized regression.\n",
        "\n",
        "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Regularization)\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Regularization_(mathematics))\n",
        "\n",
        "There are several types of regularization penalties that can be applied. Let' s test two different types of regularized regression models:\n",
        "\n",
        "1.   Ridge Regression (a.k.a. L2 regularization)\n",
        "2.   Lasso Regression (a.k.a. L1 regularization)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scZ1RoAdKFQM"
      },
      "source": [
        "### Ridge Regression\n",
        "\n",
        "**Ridge Regression** uses a penalty given by a regularization parameter (which we'll call alpha) multiplied by the sum of the squared model coefficients:\n",
        "$$\n",
        "P = \\alpha\\sum_{i=1}^{k}\\beta_i^{2}\n",
        "$$\n",
        "\n",
        "This penalty is then added to the normal RSS term. Addition of this penalty term has the effect of shrinking the model coefficients, helping to reduce model complexity and prevent overfitting.\n",
        "\n",
        "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Ridge-regression-($L_2$-Regularization))\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
        "* [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) documentation\n",
        "\n",
        "Let's look how Ridge Regression performs using all the features. Note that you can tune the weight of the penalty by adjusting the alphaValueRidge variable in the code below. Reducing alpha to 0 makes the penalty 0 and turns this model back into a normal multiple regression.\n",
        "\n",
        "Run the code block below perform Ridge Regression analysis. (Start with the default value of alpha, but later you can test how changing alpha affects the results.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4PFs6jYKmGK"
      },
      "outputs": [],
      "source": [
        "# set the alpha value\n",
        "alphaValueRidge = 1\n",
        "\n",
        "# train a Ridge Regression model\n",
        "rid_regressor = Ridge(alpha=alphaValueRidge)\n",
        "rid_regressor.fit(Xtrain, ytrain)\n",
        "y_pred_rid = rid_regressor.predict(Xtest)\n",
        "\n",
        "# calculate ridge metrics\n",
        "metrics_ridge = ModelEvaluation(ytest, y_pred_rid)\n",
        "# create a DataFrame containing ridge metrics\n",
        "pd.DataFrame({'Ridge':metrics_ridge}).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GOvfvymCua2"
      },
      "source": [
        "Now let's plot the actual vs. predicted wavelength  for the test dataset with the Ridge Regression model with `alphaValueRidge = 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF8T07IbBunp"
      },
      "outputs": [],
      "source": [
        "# plot the actual vs. predicted wavelength values\n",
        "title = \"Actual Wavelength vs. Predicted: Ridge Regression\"\n",
        "ResultsScatterPlot(y_pred_rid, ytest, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OkY2qUsTDmL"
      },
      "source": [
        "Now try tuning the value of alpha and see how it affects the results. Add a code block below to train new Ridge Regression models and test different values of alpha. (Hint: you will need to rename the variables `rid_regressor`, `y_pred_rid`, and `metrics_ridge` for each model):\n",
        "\n",
        "1.   Test alpha = 0\n",
        "2.   Test a few intermediate values of alpha\n",
        "\n",
        "Calculate the performance metrics for each model and compare to the results for an unpenalized multiple linear regression and the Ridge Regression with the highest penalty (alpha = 1). Which model gives the best performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmSj1CgpDID9"
      },
      "source": [
        "**ADD CODE BLOCKS HERE TO PERFORM RIDGE REGRESSION ANALYSIS USING DIFFERENT VALUES OF ALPHA. ALSO PLOT THE RESULTS AND CALCULATE PERFORMANCE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEGrqf-hGIR1"
      },
      "outputs": [],
      "source": [
        "# set the alpha value\n",
        "alphaValueRidge = 0.3\n",
        "\n",
        "# train a Ridge Regression model\n",
        "rid_regressor_best = Ridge(alpha=alphaValueRidge)\n",
        "rid_regressor_best.fit(Xtrain, ytrain)\n",
        "y_pred_rid = rid_regressor_best.predict(Xtest)\n",
        "\n",
        "# calculate ridge metrics\n",
        "metrics_ridge_best = ModelEvaluation(ytest, y_pred_rid)\n",
        "# create a DataFrame containing ridge metrics\n",
        "pd.DataFrame({'Ridge':metrics_ridge_best}).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPk7PmIXKIJM"
      },
      "source": [
        "### Lasso Regression\n",
        "**Lasso Regression** uses a penalty given by a regularization parameter (again we'll call it alpha) multiplied by the absolute value of the model coefficients:\n",
        "\n",
        "$$\n",
        "P = \\alpha\\sum_{i=1}^{k}|\\beta_i|\n",
        "$$\n",
        "\n",
        "Like for Ridge regression, this penalty term is added to the RSS term and has the effect of shrinking the model coefficients.\n",
        "\n",
        "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Lasso-regression-($L_1$-regularization))\n",
        "* [Wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
        "* [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) documentation\n",
        "\n",
        "\n",
        "The syntax for Lasso Regression is the same as for Ridge Regression, but substitute `Lasso(...)` for `Ridge(...)` in the code above. Try writing a code block to train a Lasso Regression model, plot the actual vs. predicted wavelength, and calculate the performance metrics. Start with the maximum penalty value (alpha = 1) and then test a few different values, as you did for Ridge Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d880LvzUVnc"
      },
      "source": [
        "**ADD A CODE BLOCK BELOW TO TRAIN A LASSO REGRESSION MODEL, PLOT THE RESULTS, AND CALCULATE PERFORMANCE METRICS. THEN DO THE SAME THING FOR A FEW DIFFERENT VALUES OF THE ALPHA PARAMETER.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the alpha value\n",
        "alphaValueLasso = 1\n",
        "\n",
        "# train a Lasso Regression model\n",
        "las_regressor = Lasso(alpha=alphaValueLasso)\n",
        "las_regressor.fit(Xtrain, ytrain)\n",
        "y_pred_las = las_regressor.predict(Xtest)\n",
        "\n",
        "# calculate lasso metrics\n",
        "metrics_lasso = ModelEvaluation(ytest, y_pred_las)\n",
        "# create a DataFrame containing lasso metrics\n",
        "pd.DataFrame({'Lasso':metrics_lasso}).transpose()"
      ],
      "metadata": {
        "id": "_FwxThh_KYC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the alpha value\n",
        "alphaValueLasso = 0.13\n",
        "\n",
        "# train a Lasso Regression model\n",
        "las_regressor_best = Lasso(alpha=alphaValueLasso)\n",
        "las_regressor_best.fit(Xtrain, ytrain)\n",
        "y_pred_las = las_regressor_best.predict(Xtest)\n",
        "\n",
        "# calculate lasso metrics\n",
        "metrics_lasso_best = ModelEvaluation(ytest, y_pred_las)\n",
        "# create a DataFrame containing lasso metrics\n",
        "pd.DataFrame({'Lasso':metrics_lasso_best}).transpose()"
      ],
      "metadata": {
        "id": "aO7zTqbUG9jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GE7CEMWOrlN"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Let's now review all the developed models to see which perform the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZgTUeDZ7aiy"
      },
      "outputs": [],
      "source": [
        "# compile all the metrics\n",
        "AllTheMetrics = {'Simple_LR': metrics_simple_lr,\n",
        "                 'Multiple_LR': metrics_multiple_lr,\n",
        "                 'Ridge (alpha = 1)': metrics_ridge,\n",
        "                }\n",
        "# create a DataFrame containing all the metrics\n",
        "pd.DataFrame(AllTheMetrics).transpose().sort_values('MSE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVM6cc_xUl2e"
      },
      "source": [
        "**MODIFY THE CODE BLOCK ABOVE TO ADD RESULTS FOR YOUR NEW MODELS:**\n",
        "1.   SIMPLE LINEAR REGRESSION WITH THE BEST CORRELATED FEATURE\n",
        "2.   RIDGE REGRESSION WITH ALPHA = 0\n",
        "3.   RIDGE REGRESSION WITH THE BEST INTERMEDIATE VALUE OF ALPHA\n",
        "4.   LASSO REGRESSION WITH ALPHA = 1\n",
        "5.   LASSO REGRESSION WITH ALPHA = 0\n",
        "6.   LASSO REGRESSION WITH THE BEST INTERMEDIATE VALUE OF ALPHA\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AllTheMetrics = {'Simple_LR_Best_Corr': metrics_simple_lr3,\n",
        "                 'Ridge (alpha = 1)': metrics_ridge,\n",
        "                 'Ridge (alpha = Best)': metrics_ridge_best,\n",
        "                 'Lasso (alpha = 1)': metrics_lasso,\n",
        "                 'Lasso (alpha = Best)': metrics_lasso_best}\n",
        "# create a DataFrame containing all the metrics\n",
        "pd.DataFrame(AllTheMetrics).transpose().sort_values('MSE')"
      ],
      "metadata": {
        "id": "KZaKIVKgIYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o4HnuQ4Uzm6"
      },
      "source": [
        "Ensure that the table above includes all the models that you tested.\n",
        "\n",
        "Although the model performance isn't perfect, you can see that choosing a well-correlated feature can give better results than a poorly-correlated feature, and that using a penalized regression model can often improve on the performance of an unpenalized multiple regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R7bHHevsH5L"
      },
      "source": [
        "## Feature Importance\n",
        "In addition to using a regression model to make quantitative predictions of our target value, we can also use it to gain insight into the system that we are studying. In other words, we can use regression models to generate hypotheses about what factors are important for cyanine dye absorption. One way to do this is to assess which of the independent variables make the largest contribution to determining the predicted result in our regression models. We refer to this property as feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhqexQBkRY64"
      },
      "source": [
        "### Feature Importance Using Coefficients\n",
        "\n",
        "There is a simple approach for interpreting feature importance in linear regression models with normalized features, like the ones we've used so far. Coefficients with larger magnitudes correspond to features that are playing a larger role in the model.\n",
        "\n",
        "Run the code blocks below to plot the coefficients associated with each feature in the multiple regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Mo9hcIDEnQ"
      },
      "outputs": [],
      "source": [
        "feature_importance_lr = pd.DataFrame({'Feature':regressor_mr.feature_names_in_,\n",
        "                                      'Coef': regressor_mr.coef_,\n",
        "                                      'Abs Coef': np.abs(regressor_mr.coef_)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shy7qRGMRya3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.barplot(data = feature_importance_lr, x='Coef', y='Feature', color='limegreen')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ItO7-2D1Ynx"
      },
      "source": [
        "To make the results even clearer, we can sort by decreasing coefficient value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xilItIczJbpS"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.barplot(data = feature_importance_lr.sort_values(\"Coef\",ascending=False), x='Coef', y='Feature', color='limegreen');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laa85i8XJQvJ"
      },
      "outputs": [],
      "source": [
        "feature_importance_lr = pd.DataFrame({'Feature':regressor_mr.feature_names_in_,\n",
        "                                      'Coef': regressor_mr.coef_,\n",
        "                                      'Abs Coef': np.abs(regressor_mr.coef_)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQiyY8uhAiyQ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
        "sns.barplot(data = feature_importance_lr, x='Feature', y='Abs Coef', order=feature_importance_lr.sort_values('Abs Coef', ascending=False)['Feature'], color='limegreen')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njcMdWkM1luy"
      },
      "source": [
        "You might notice that the features with the highest weights are not the same as the features that were best correlated with the MaxAbsorbanceWavelength according to the Pearson Correlation analysis. That discrepancy can arise because the Pearson Correlation analysis looks at the correlation between each independent variable and the dependent variable separately, whereas this analysis is looking at the contribution of each independent variable to the model that includes all 13 independent variables together.\n",
        "\n",
        "Now let's look at the feature importance for the Ridge and Lasso Regression models. Modify the code blocks above to generate plots of the coefficients for each of those models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tCGz_rT2Ps_"
      },
      "source": [
        "**ADD A CODE BLOCK BELOW TO PLOT THE FEATURE COEFFICIENTS FOR THE RIDGE AND LASSO REGRESSION MODELS WITH ALPHA = 1**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "feature_importance_ridge = pd.DataFrame({'Feature':rid_regressor.feature_names_in_,\n",
        "                                      'Coef': rid_regressor.coef_,\n",
        "                                      'Abs Coef': np.abs(rid_regressor.coef_)})\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.barplot(data = feature_importance_ridge.sort_values(\"Coef\",ascending=False), x='Coef', y='Feature', color='limegreen');"
      ],
      "metadata": {
        "id": "YM43A1TTIkWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solution\n",
        "feature_importance_lasso = pd.DataFrame({'Feature':las_regressor.feature_names_in_,\n",
        "                                      'Coef': las_regressor.coef_,\n",
        "                                      'Abs Coef': np.abs(las_regressor.coef_)})\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "sns.barplot(data = feature_importance_lasso.sort_values(\"Coef\",ascending=False), x='Coef', y='Feature', color='limegreen');"
      ],
      "metadata": {
        "id": "FcFv3HTzIolq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGiIj6Z2jcN"
      },
      "source": [
        "## Gaining Insight from Regression Models\n",
        "\n",
        "Look back at the results of all the regression models that you've trained so far. Also consider the Pearson Correlation analysis and the feature importance analysis.\n",
        "\n",
        "None of these features can fully explain the differences in absorbance wavelength for all the cyanine dyes. But now, use your chemical intuition and try to generate a hypothesis that is grounded in quantum chemistry, using any insight that you can take from the regression models. (Hint: think about the quantum chemistry models that you've studied so far in your physical chemistry course.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHF2EYd49c5k"
      },
      "source": [
        "# Part II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXbbRMGBEeGI"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "So far we've only considered the 13 features in our dataset. But we can also make combinations of these individual features to try to improve model performance. This process of creating new features from the ones we already have, either combining them or applying other operations, is called **feature engineering**.\n",
        "\n",
        "*  [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)\n",
        "*  [Wikipedia](https://en.wikipedia.org/wiki/Feature_engineering)\n",
        "\n",
        "As we saw in the [Get and Preprocess the Data section](#scrollTo=7LbxW6isKPpb&line=1&uniqifier=1), our tables are stored in Python variables. Given that our tables are saved in variables, we can experiment with them and then keep our changes in new variables or update the ones we have.\n",
        "\n",
        "If we would like to create a new column using another one that we already have, we can simply state the name of the new column and what operation we want to apply by following the structure:\n",
        "\n",
        "`table['new_column'] = table['column_1'] + table['column_2']`\n",
        "\n",
        "or\n",
        "\n",
        "`table['new_column'] = table['column_1'] * table['column_2']`\n",
        "\n",
        "or any other arithmetic operation.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "We are working with `grades` that contains all students' grades from class A, B, and C.\n",
        "\n",
        "|Student | Class_A | Class_B| Class_C|\n",
        "-----|-----|-----| -----|\n",
        "|Student 1|91|95|78|\n",
        "|Student 2|96|87|90|\n",
        "|Student 3|79|89|92|\n",
        "|Student 4|86|72|80|\n",
        "\n",
        "We have been asked to determine the average grade for each student and store the result as a new feature called `Average_Grade`. To obtain this number, we can do the following:\n",
        "\n",
        "`grades['Average_Grade'] = (grades['Class_A'] + grades['Class_B'] + grades['Class_C'])/3`\n",
        "\n",
        "And consequently, we obtain the following result:\n",
        "\n",
        "|Student | Class_A | Class_B| Class_C| Average_Grade|\n",
        "-----|-----|-----|-----|-----|\n",
        "|Student 1|91|95|78|88|\n",
        "|Student 2|96|87|90|91|\n",
        "|Student 3|79|89|92|87|\n",
        "|Student 4|86|72|80|83|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSbujNlWxVDp"
      },
      "outputs": [],
      "source": [
        "# Create a function that predicts and generates the metrics for our experiments\n",
        "def predict_and_score(Train_data, Test_data, y_train, y_test):\n",
        "  regressor = LinearRegression()\n",
        "  regressor.fit(Train_data, y_train)\n",
        "  # predicting the test set results\n",
        "  y_pred = regressor.predict(Test_data)\n",
        "  return pd.DataFrame({'Experiment': ModelEvaluation(y_test, y_pred)}).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBXBdgpLM2Pw"
      },
      "source": [
        "Let's try making a new feature that is the product of the number of heteroatoms and the number of heterocycles in the molecules and then use that new feature in a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el8Z-L3V08ie"
      },
      "outputs": [],
      "source": [
        "# Example new feature #1: HeteroatomCount * HeterocycleCount\n",
        "exp1_train = Xtrain.copy()\n",
        "exp1_test = Xtest.copy()\n",
        "\n",
        "# HACxHCC = HeteroatomCount * HeterocycleCount\n",
        "exp1_train['HACxHCC'] = exp1_train['HeteroatomCount'] * exp1_train['HeterocycleCount']\n",
        "exp1_test['HACxHCC'] = exp1_test['HeteroatomCount'] * exp1_test['HeterocycleCount']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c0sGikNFHM"
      },
      "source": [
        "Now repeat this process by combining two or more features of your choice (by addition, multiplication, subtraction, etc). For example, you could pick two of the features with highest correlation in the Pearson analysis, or you could pick two features that your chemical intuition says might be important. Add a code block below to generate a new feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UDWpIalPz6U"
      },
      "source": [
        "**ADD A CODE BLOCK BELOW TO CREATE A NEW FEATURE BY COMBINING FEATURES FROM THE ORIGINAL DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split our dataset into two sets: training dataset and test dataset, keeping 20% of the data for testing\n",
        "X2 = X.copy()\n",
        "y2 = y.copy()\n",
        "X2[\"LinkerLengthxFractionCarbonSP3\"] = X2[\"LinkerLength\"] * X2[\"FractionCarbonSP3\"]"
      ],
      "metadata": {
        "id": "bcs2n768IrNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Rvl_0DQZ9X"
      },
      "source": [
        "Now try running simple linear regression analyses separately with HACxHCC and your new feature. You will need to go through the preprocessing data steps (separating x and y values, splitting, rescaling, etc) before performing the regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cLKuRgLNzmi"
      },
      "source": [
        "**ADD A CODE BLOCK BELOW TO REPEAT THE DATA PREPROCESSING, PERFORM A SIMPLE LINEAR REGRESSION FOR THE TWO NEW FEATURES, VISUALIZE THE RESULTS, AND CALCULATE PERFORMANCE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain2, Xtest2, ytrain2, ytest2 = train_test_split(X2, y2, test_size =0.20,random_state = 24)\n",
        "\n",
        "# import the MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# We begin by determining the maximum and minimum of each feature:\n",
        "XtrainScaled2 = scaler.fit_transform(Xtrain2)\n",
        "XtestScaled2 = scaler.transform(Xtest2)\n",
        "\n",
        "Xtrain2 = pd.DataFrame(XtrainScaled2, index=Xtrain2.index, columns= Xtrain2.columns)\n",
        "Xtest2 = pd.DataFrame(XtestScaled2, index=Xtest2.index, columns= Xtest2.columns)\n",
        "\n",
        "# select a single feature\n",
        "featureNameFE = 'LinkerLengthxFractionCarbonSP3'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(Xtrain2[featureNameFE].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(Xtest2[featureNameFE].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressorFE = LinearRegression()\n",
        "regressorFE.fit(single_feature_lr_X_train, ytrain2)\n",
        "# predicting the test set results\n",
        "y_pred_simple_lr_FE = regressorFE.predict(single_feature_lr_X_test)\n",
        "# Calculate the performance metrics for the FractionCarbonSP3 simple linear regression using the corresponding single - variable test data\n",
        "metrics_simple_lr_FE = ModelEvaluation(ytest2, y_pred_simple_lr_FE)\n",
        "\n",
        "# turn our metric results into a pandas dataset\n",
        "pd.DataFrame({f'Simple Linear Regression FE: {featureNameFE}':metrics_simple_lr_FE})"
      ],
      "metadata": {
        "id": "411FT3p9IrkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureNameFE = 'HACxHCC'\n",
        "\n",
        "# Save the training and test data for that feature in the corresponding association variables\n",
        "single_feature_lr_X_train = np.reshape(exp1_train[featureNameFE].values, (-1,1))\n",
        "single_feature_lr_X_test = np.reshape(exp1_test[featureNameFE].values, (-1,1))\n",
        "\n",
        "# Train a simple linear regression for that feature and save it to the association variable\n",
        "regressorFE = LinearRegression()\n",
        "regressorFE.fit(single_feature_lr_X_train, ytrain2)\n",
        "# predicting the test set results\n",
        "y_pred_simple_lr_FE = regressorFE.predict(single_feature_lr_X_test)\n",
        "# Calculate the performance metrics for the simple linear regression using the corresponding single - variable test data\n",
        "metrics_simple_lr_FE = ModelEvaluation(ytest2, y_pred_simple_lr_FE)\n",
        "\n",
        "# turn our metric results into a pandas dataset\n",
        "pd.DataFrame({f'Simple Linear Regression FE: {featureNameFE}':metrics_simple_lr_FE})"
      ],
      "metadata": {
        "id": "L5NgfxIqZPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhoNEdXSN8E8"
      },
      "source": [
        "How well did HACxHCC and your new feature perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COTNAsOkGdXF"
      },
      "source": [
        "## Tree Models\n",
        "\n",
        "Tree models make decisions using a tree-like structure with nodes, branches, and leaves. They can be used for either regression or classification analyses.\n",
        "\n",
        "The most common tree-based models for regression problems are:\n",
        "\n",
        "1.   Decision Tree\n",
        "2.   Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0q-ouchGZU0"
      },
      "source": [
        "### Decision Tree Regression\n",
        "\n",
        "The Decision Tree algorithm uses a single tree to make predictions. It models the data in a tree structure in which each leaf node leads to the prediction for the dependent variable. Although easy to implement, Decision Tree models can be prone to overfitting.\n",
        "\n",
        "- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
        "- [scikit-learn](https://scikit-learn.org/stable/modules/tree.html#regression) documentation\n",
        "\n",
        "The code block below trains a Decision Tree model and visualizes the tree structure. Run this analysis, then add a code block to plot the actual vs. predicted wavelength and to calculate the performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the DecisionTree module\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Fit a DecisionTree model to the dataset\n",
        "DTReg = DecisionTreeRegressor()\n",
        "DTReg.fit(Xtrain, ytrain)\n",
        "y_pred_DTReg = DTReg.predict(Xtest)"
      ],
      "metadata": {
        "id": "2wOrvY3ju_dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize how well this model fits the training data:"
      ],
      "metadata": {
        "id": "0oMMgrq7u7YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the actual vs. predicted wavelength values\n",
        "title = \"Actual Wavelength vs. Predicted: Decision Tree Regression\"\n",
        "ResultsScatterPlot(DTReg.predict(Xtrain), ytrain, title)"
      ],
      "metadata": {
        "id": "7Klr0GnvvIiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize how well this model predicts the test data:"
      ],
      "metadata": {
        "id": "RyY4ahCHvHh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbtn7gkWIPu0"
      },
      "outputs": [],
      "source": [
        "# plot the actual vs. predicted wavelength values\n",
        "title = \"Actual Wavelength vs. Predicted: Decision Tree Regression\"\n",
        "ResultsScatterPlot(y_pred_DTReg, ytest, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR5EawVyR9Km"
      },
      "source": [
        "Now add a code block to plot and store the performance metrics for the Decision Tree regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ49H8t-BSLh"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO PLOT THE DECISION TREE RESULTS AND CALCULATE PERFORMANCE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "metrics_DT = ModelEvaluation(ytest, y_pred_DTReg)\n",
        "\n",
        "# turn our metric results into a pandas dataset\n",
        "pd.DataFrame({f'Decision Tree':metrics_DT})"
      ],
      "metadata": {
        "id": "70UvkUsmUw0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SBiD0XqIrO0"
      },
      "source": [
        "### Random Forest Regression\n",
        "\n",
        "Random Forest builds a collection of Decision Trees, where each tree is trained with a random subset of training instances and a random subset of features. The final prediction is determined as the average output among the collection of trees. The Random Forest algorithm is less prone to overfitting data than the Decision Tree algorithm.\n",
        "\n",
        "- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html#Random-Forest-Regression)\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
        "- [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) documentation\n",
        "\n",
        "The code block below trains a Random Forest model. Run this analysis, then add a code block to plot the actual vs. predicted wavelength and to calculate the performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwIIqquhI2C1"
      },
      "outputs": [],
      "source": [
        "# Import the RandomForestRegressor module\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Fit a Random Forest model to the dataset\n",
        "randomF = RandomForestRegressor(n_estimators = 200, random_state = 12)\n",
        "randomF.fit(Xtrain, ytrain)\n",
        "y_pred_rf = randomF.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV_6q3q1BkMH"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO PLOT THE RANDOM FOREST RESULTS AND CALCULATE PERFORMANCE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_RF = ModelEvaluation(ytest, y_pred_rf)\n",
        "\n",
        "# turn our metric results into a pandas dataset\n",
        "pd.DataFrame({f'Decision Tree':metrics_RF})"
      ],
      "metadata": {
        "id": "isLF7m2hbDYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6oFL6XvJXOr"
      },
      "source": [
        "How do the Random Forest and Decision Tree results compare to the models tested above? Add a code block below to make a new table of performance metrics that includes these models as well as all the models tested above. (Hint: you can copy and modify the table from the [Model Evaluation](#scrollTo=4i7by65Xx4gX) section.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlzBIxn9BsFn"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO GENERATE A NEW TABLE WITH PERFORMANCE METRICS FOR ALL TESTED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({\"SLR\":metrics_simple_lr,\n",
        "              \"LR\":metrics_multiple_lr,\n",
        "              \"Ridge\":metrics_ridge,\n",
        "              \"Ridge_BEST\":metrics_ridge_best,\n",
        "              \"Lasso\":metrics_lasso,\n",
        "              \"Lasso_BEST\":metrics_lasso_best,\n",
        "              \"DT\":metrics_DT,\n",
        "              \"RF\":metrics_RF}).transpose().sort_values(by=\"MAE\")"
      ],
      "metadata": {
        "id": "9alOBkY-bT18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8ulRc1uJ6kd"
      },
      "source": [
        "## Feature Importance Using SHapley Additive exPlanations (SHAP)\n",
        "\n",
        "For the linear regression models in Part I of this activity, we used the coefficients to determine which features were most important in the model. But for non-linear models, like DecisionTree or RandomForest, determining feature importance can be more difficult. There are a few model-specific approaches that can be used, but here we will implement a general approach called SHapley Additive exPlanations (SHAP).\n",
        "\n",
        "SHAP is a method derived from Game Theory (a topic in Economics) that brings interpretability to machine learning models, showing the contribution of each feature in a model-agnostic way. For more information see:\n",
        "\n",
        "* Original report (by Scott M. Lundberg and Su-In Lee): [A Unified Approach to Interpreting Model Predictions](https://papers.nips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html)\n",
        "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "Run the code blocks below to import the SHAP module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD8LHijrKrPM"
      },
      "outputs": [],
      "source": [
        "# Run this if you do not have the shap library installed\n",
        "!pip install -q shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duDfdpGpKsjX"
      },
      "outputs": [],
      "source": [
        "shap.initjs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL6y0YDpLS9a"
      },
      "source": [
        "### SHAP Analysis for Decision Tree Regression\n",
        "Now let's try to understand our Decision Tree model using the SHAP approach. The code block below performs SHAP analysis on our Decision Tree model and displays a plot of feature importance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaLf3gSNKgtd"
      },
      "outputs": [],
      "source": [
        "# load SHAP analysis on our Decision Tree Model\n",
        "explainer = shap.TreeExplainer(DTReg)\n",
        "shap_values_DTReg = explainer.shap_values(Xtrain)\n",
        "\n",
        "# plot SHAP graph\n",
        "shap.summary_plot(shap_values_DTReg, Xtrain);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loCJMsVpLvgD"
      },
      "source": [
        "The plot above can be interpreted by looking at the distance from the Feature Impact = 0 line and the displayed color of each point. The distance from the 0 line reflects how much of an impact the feature had on the model for that particular data point. The displayed color shows whether the feature value was low/more negative (*blue*) or high/more positive (*red*) for that point. Features with points more distant from the 0 line were more important for the model. When points of the same color cluster on one side of the 0 line, it indicates that the feature value tended to shift the model prediction in a more consistent direction.\n",
        "\n",
        "It can be simpler to omit the effect direction and just calculate the mean absolute SHAP value for each feature and display them in a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vcKQ64IK778"
      },
      "outputs": [],
      "source": [
        "# display SHAP in a barchart presenting the impact of each feature\n",
        "shap.summary_plot(shap_values_DTReg, X, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9fCpHO_L_RS"
      },
      "source": [
        "### SHAP Analysis for Random Forest Regression\n",
        "Now follow the same approach to interpret our Random Forest model using the SHAP approach. Add a code block below to perform SHAP analysis and plot the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdKwPGJLMLAq"
      },
      "source": [
        "**ADD A CODE BLOCK HERE TO PERFORM SHAP ANALYSIS FOR THE RANDOM FOREST MODEL AND GENERATE PLOTS OF FEATURE IMPORTANCE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load SHAP analysis on our Decision Tree Model\n",
        "explainer = shap.TreeExplainer(randomF)\n",
        "shap_values_RandomF = explainer.shap_values(Xtrain)\n",
        "\n",
        "# plot SHAP graph\n",
        "shap.summary_plot(shap_values_RandomF, Xtrain)"
      ],
      "metadata": {
        "id": "ljzf6beFI1Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display SHAP in a barchart presenting the impact of each feature\n",
        "shap.summary_plot(shap_values_RandomF, X, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "p_RIpWzsdN_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZpUDVMMQ6K"
      },
      "source": [
        "Are these tree models relying on the same features as the linear regression models above?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK323UJCCFT-"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This exercise is just one example of how machine learning regression analysis can be applied to chemistry. As you continue in your chemistry studies or research, look for other opportunities to make predictions or generate hypotheses using these types of models!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}